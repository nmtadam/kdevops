# -*- mode: ruby -*-
# # vi: set ft=ruby :
# Automatically generated file by kdevops {{ kdevops_version }}

# This file should only be edited to *grow* support for new features.
# If you are not interested in that and are happy with what is provided,
# all you'd have to do is edit your respective project name space
# yaml file to set the hosts / playbooks you need to run.
#
# This file is automatically created for you from a jinja2 template file
# and parsed with ansible after extra_vars.yaml file is created. The source is
# playbooks/roles/gen_nodes/templates/Vagrantfile.j2

Vagrant.require_version ">= 1.6.0"

require 'json'
require 'yaml'
require 'fileutils'
require 'rbconfig'

@os = RbConfig::CONFIG['host_os']

# XXX: upstream libvirt enhancement needed
#
# Vagrant allows multiple provider code to be supported easily, however
# this logic assuems all provider setup is supported through configuration
# variables from the provider. This is not the case for libvirt. We need
# to create the qemu image, and the libvirt vagrant provider doesn't have
# support to create the images as virtual box does. We run the commands
# natively, however this also reveals that on execution path even code
# for other providers gets executed regardless of the provider you are
# using *unless* that code is using provider specific variables. Best we
# can do for now is detect your OS and set a local variable where we *do*
# run different local code paths depending on the target provider.
#
# Right now we make these assumptions:
#
# Your OS            Provider
# -------------------------------
# Linux              libirt
# Mac OS X           virtualbox
provider = "libvirt"

# Are we in a nested virtualized environment? For now we just
# do something different if we're not on bare metal. We may want
# to do something different later to optimize this further but
# we have to draw the line somewhere.
nested = %x(which systemd-detect-virtd 1>/dev/null 2>&1 || systemd-detect-virt) != "none"

case
when @os.downcase.include?('linux')
  provider = "libvirt"
when @os.downcase.include?('darwin')
  provider = "virtualbox"
else
  puts "You OS hasn't been tested yet, go add support and send a patch."
  exit
end

# We assume if you are in kdevops/vagrant/ your project namespace is kdevops
project_namespace = File.basename(File.dirname(Dir.getwd))

node_config          =  "kdevops_nodes.yaml"
node_config_override =  "kdevops_nodes_override.yaml"

config_data = YAML.load_file(node_config)

override_data = false
if File.file?(node_config_override)
  override_data = true
  config_data_override = YAML.load_file(node_config_override)
end

global_data = config_data['vagrant_global']
vagrant_boxes = config_data['vagrant_boxes']

if override_data
  if config_data_override['vagrant_global']
    global_data = config_data_override['vagrant_global']
  end
  if config_data_override['vagrant_boxes']
    vagrant_boxes = config_data_override['vagrant_boxes']
  end
end

supported_provider = case provider
  when "virtualbox" then true
  when "libvirt" then true
  else false
end

if ! supported_provider
  puts "Unsupported provider: #{provider} on " + RbConfig::CONFIG['host_os']
  puts "Consider adding support and send a patch"
  exit
end

qemu_group = global_data['libvirt_cfg']['qemu_group']
qemu_group_auto = global_data['libvirt_cfg']['qemu_group_auto']

if qemu_group_auto
  if File.exists?('/etc/debian_version')
    qemu_group = 'libvirt-qemu'
  else
    qemu_group = 'qemu'
  end
end

kdevops_pool_path = global_data['storage_pool_path']

libvirt_session_public_network_dev = global_data['libvirt_cfg']['session_public_network_dev']

Vagrant.configure("2") do |config|
  vagrant_boxes.each do |server_data|
    # Using sync folders won't work for say openstack / aws / azure / gce, and
    # for that we'll use terraform, so best to allow whatever data we want to
    # sync be provisioned with ansible.
    config.vm.synced_folder './', '/vagrant', type: '9p', disabled: true, accessmode: "mapped", mount: false
    config.vm.define server_data["name"] do |srv|
      srv.vm.box = server_data['box'] ? server_data['box'] : global_data["box"]
      server_box_version = server_data['box_version'] ? server_data['box_version'] : global_data["box_version"]
      if server_box_version != ""
        srv.vm.box_version = server_box_version
      end
{% if libvirt_session %}
      # https://wiki.libvirt.org/page/FAQ#What_is_the_difference_between_qemu:.2F.2F.2Fsystem_and_qemu:.2F.2F.2Fsession.3F_Which_one_should_I_use.3F
      # qemu:///session has a serious drawback: since the libvirtd instance
      # does not have sufficient privileges, the only out of the box network
      # option is qemu's usermode networking, which has nonobvious
      # limitations, so its usage is discouraged. More info on qemu networking
      # options: http://people.gnome.org/~markmc/qemu-networking.html
      srv.vm.network "public_network", :type => "user"
{% else %}
      if ! nested
        srv.vm.network "private_network", ip: server_data["ip"]
      end
{% endif %}
      host_name = server_data["name"]
      node_custom_data_path = kdevops_pool_path + "/" + "/#{host_name}"
      FileUtils.mkdir_p(node_custom_data_path) unless File.exists?(node_custom_data_path)
      nvme_path = "#{node_custom_data_path}/" + global_data['nvme_disk_path']
      FileUtils.mkdir_p(nvme_path) unless File.exists?(nvme_path)
{% if libvirt_enable_cxl %}
      cxl_path = "#{node_custom_data_path}" + "/cxl"
      FileUtils.mkdir_p(cxl_path) unless File.exists?(cxl_path)
{% endif %}
{% if virtualbox_provider %}
      srv.vm.provider "virtualbox" do |vb, override|
	if provider == "virtualbox" && ! global_data['virtualbox_cfg']['auto_update']
          # we'll need to run later: vagrant vbguest install
          config.vbguest.auto_update = false
        end
        override.vm.hostname = host_name
        override.vm.boot_timeout = global_data['boot_timeout']
        vb.memory = global_data["memory"]
        vb.cpus = global_data["cpus"]
        if global_data['nvme_disks']
          port = 0
          port_count = global_data['nvme_disks'].size()
          global_data['nvme_disks'].each do |key, value|
            size = value['size']
	    # XXX: the purpose of the nvme drive is unused for now but later we
	    # can let ansible provision it for our target mount paths
            purpose = key
            port_plus = port + 1
            nvme_disk_postfix = global_data['virtualbox_cfg']['nvme_disk_postfix']
            nvme_disk = nvme_path + "/nvme#{port}n#{port_plus}.#{nvme_disk_postfix}"

            # "Standard" provides a sparse file. That's what we want, we cheat
            # the OS and only use what we need. If you want the real file size
            # add a global config option and send a patch and justify it. I'd
            # like to hear about it. We use sparse files for libvirt as well
            # and should try to keep setup in sync.
            if (! File.file?(nvme_disk))
              vb.customize ["createmedium", "disk", "--filename", nvme_disk, "--variant", "Standard", "--format", nvme_disk_postfix.upcase, "--size", size]
            end
            # Virtualbox supports only one nvme controller... this will fail
            # unless you are a Virtualbox hacker adding support for this :)
            if global_data['virtualbox_cfg']['nvme_controller_per_disk']
              # https://www.virtualbox.org/manual/ch08.html#vboxmanage-storagectl
              # This command attaches, modifies, and removes a storage
              # controller. After this, virtual media can be attached to the
              # controller with the storageattach command.
              nvme_name = "nvme#{port}"
              if (! File.file?(nvme_disk))
                vb.customize ["storagectl", :id, "--name", "#{nvme_name}", "--add", "pcie", "--controller", "NVMe", "--portcount", 1, "--bootable", "off"]
              end
	      # Now attach the drive
	      vb.customize ["storageattach", :id, "--storagectl", "#{nvme_name}", "--type", "hdd", "--medium", nvme_disk, "--port", 0]
            else
              if (port == 0 && !File.file?(nvme_disk))
                vb.customize ["storagectl", :id, "--name", "nvme0", "--add", "pcie", "--controller", "NVMe", "--portcount", port + 1, "--bootable", "off"]
              end
	      vb.customize ["storageattach", :id, "--storagectl", "nvme0", "--type", "hdd", "--medium", nvme_disk, "--port", port]
            end

            if global_data['enable_sse4']
	      # Support for the SSE4.x instruction is required in some versions of VB.
              vb.customize ["setextradata", :id, "VBoxInternal/CPUM/SSE4.1", "1"]
              vb.customize ["setextradata", :id, "VBoxInternal/CPUM/SSE4.2", "1"]
            end
            port += 1
          end # end of looping all nvme disks
        end # end of checking for nvme disks
      end # end of virtualbox provider section
{% endif %}
{% if libvirt_provider %}
      # For details see: https://github.com/vagrant-libvirt/vagrant-libvirt
      srv.vm.provider "libvirt" do |libvirt, override|
        #libvirt.host = "localhost"
        override.vm.hostname = host_name
        override.vm.boot_timeout = global_data['boot_timeout']
        libvirt.watchdog :model => 'i6300esb', :action => 'reset'
        libvirt.storage_pool_path = kdevops_pool_path
{% if libvirt_host_passthrough %}
        libvirt.cpu_mode = 'host-passthrough'
{% endif %}
{% if libvirt_storage_pool_create %}
	libvirt.storage_pool_name = '{{ libvirt_storage_pool_name }}'
{% endif %}
{% if libvirt_session %}
	libvirt.qemu_use_session = true
	libvirt.uri = global_data['libvirt_cfg']['uri']
	libvirt.system_uri = global_data['libvirt_cfg']['system_uri']
	libvirt.socket = global_data['libvirt_cfg']['session_socket']
	libvirt.management_network_device = global_data['libvirt_cfg']['session_management_network_device']
{% else %}
	if nested
	  libvirt.management_network_name = 'vagrant-libvirt-private'
	  libvirt.management_network_address = '192.168.124.0/24'
	  libvirt.management_network_device = global_data['libvirt_cfg']['session_management_network_device']
	end
{% endif %}
        libvirt.memory = global_data["memory"]
        libvirt.cpus = global_data["cpus"]
        libvirt.emulator_path = global_data['libvirt_cfg']['emulator_path']
        if server_data["machine_type"]
          libvirt.machine_type = server_data["machine_type"]
        else
          if global_data['libvirt_cfg']['machine_type']
            libvirt.machine_type = global_data['libvirt_cfg']['machine_type']
          end
        end
        # Add an extra spare PCI or PCI-E root bus to be used for NVME drives.
        #
        # We use a dedicated PCI or PCI-E root bus to not clash with defaults
        # which libvirt may use on PCI root bus (pci.0 if PCI, pcie.0 if PCI-E).
        # For a while in kdevops we were not clashing with the defaults brought
        # up by libvirt-qemu, however recent versions seem to start clashing up
        # to pci.0,addr=0x3 so we'd see an error like:
        #
        # PCI: slot 3 function 0 not available for virtio-blk-pci, in use by nvme,id=(null) (Libvirt::Error)
        #
        # Just create a PCI or PCI-E root bus dedicated for nvme drives. We
        # use 0x08 to place this PCI / PCI-E root bus as we know this is
        # avilable on modern x86-64 systems. Eventually we may need to bump
        # this to 0x9, but it would be better instead to have vagant-libvirt
        # speak "add a new PCI or PCI-E root bus" and "add nvme drives".
        #
{% if not libvirt_override_machine_type %}
        # For i440x on x86_64 (default on libvirt as of today) we use PCI, not
        # PCI-E. Below assumes i440x. i440x cannot support CXL as it does not
        # suport PCI-E.
        libvirt.qemuargs :value => "-device"
        libvirt.qemuargs :value => "pci-bridge,id=custom-pci-for-nvme,chassis_nr=1,bus=pci.0,addr=0x8"
{% else %}
	# This is for exclusive PCIe hierarchies.
	#
        # For 'q35' on x86_64 and 'virt' for AArch64 we use can use PCI-E
        # only hierarchies.
        #
	# We add a dedicated PCI root bus (pxb-pcie) just for dedicated PCI
	# root ports for NVMe, this will be pcie.1.
	#
        # We can keep doing this to support new technologies. For instance,
        # it may be reasoanble to add a PCI-E root bus also to support PCI-E
        # hotplug on PCI-E capable systems and avoid clashes with libvirt
	# defaults on pcie.0. CXL uses its own host bridge so device
	# enumeration is pretty well isolated already so no need to do anything
	# extra there.
        libvirt.qemuargs :value => "-device"
        libvirt.qemuargs :value => "pxb-pcie,id=pcie.1,bus_nr=32,bus=pcie.0,addr=0x8"
{% if libvirt_enable_cxl %}
{% if libvirt_enable_cxl_demo_topo1 or libvirt_enable_cxl_demo_topo2 %}
        libvirt.qemuargs :value => "-machine"
        libvirt.qemuargs :value => "cxl=on"

	# CXL host bus
        libvirt.qemuargs :value => "-device"
        libvirt.qemuargs :value => "pxb-cxl,bus=pcie.0,id=cxl.0,bus_nr=52,addr=0x9"


        cxl_mem1_path = cxl_path + "/cxl-mem1.raw"
        libvirt.qemuargs :value => "-object"
        libvirt.qemuargs :value => "memory-backend-file,id=kdevops-cxl-mem1,share=on,mem-path=#{cxl_mem1_path},size=256M"

	# Label Storage Area, used to store CXL namespace labels and region labels
        cxl_lsa1_path = cxl_path + "/cxl-lsa1.raw"
        libvirt.qemuargs :value => "-object"
        libvirt.qemuargs :value => "memory-backend-file,id=kdevops-cxl-lsa1,share=on,mem-path=#{cxl_lsa1_path},size=256M"

        libvirt.qemuargs :value => "-device"
        libvirt.qemuargs :value => "cxl-rp,port=0,bus=cxl.0,id=kdevops_cxl_root_port1,chassis=0,slot=2"

	{% if libvirt_enable_cxl_demo_topo2 %}
		libvirt.qemuargs :value => "-device"
		libvirt.qemuargs :value => "cxl-rp,port=1,bus=cxl.0,id=kdevops_cxl_root_port2,chassis=0,slot=3"
	{% endif %}
        libvirt.qemuargs :value => "-device"
        libvirt.qemuargs :value => "cxl-type3,bus=kdevops_cxl_root_port1,memdev=kdevops-cxl-mem1,lsa=kdevops-cxl-lsa1,id=kdevops-cxl-pmem0"

        libvirt.qemuargs :value => "-M"
        libvirt.qemuargs :value => "cxl-fmw.0.targets.0=cxl.0,cxl-fmw.0.size=512M"
{% endif %}
{% endif %}
{% endif %}

        if server_data['pcipassthrough']
          server_data['pcipassthrough'].each do |key, value|
            libvirt.pci :domain => value['domain'], :bus => value['bus'], :slot => value['slot'], :function => value['function']
          end
        end
        if global_data['nvme_disks']
          port = 0
          port_count = global_data['nvme_disks'].size()
          global_data['nvme_disks'].each do |key, value|
            size = value['size']
            zoned = value['zoned']
            if zoned && !global_data['enable_zns']
		next
            end
	    if zoned
              logical_block_size = value['logical_block_size']
              physical_block_size = value['physical_block_size']
              zone_size = value['zone_size']
              zone_capacity = value['zone_capacity']
              zone_max_open = value['zone_max_open']
              zone_max_active = value['zone_max_active']
              zone_zasl = value['zone_zasl']
            end
            purpose = key
            port_plus = port + 1
	    key_id_prefix = global_data['libvirt_cfg']['nvme_disk_id_prefix']
	    disk_id = "#{key_id_prefix}#{port}"
	    serial_id = "serial#{port}"
            nvme_disk_postfix = global_data['libvirt_cfg']['nvme_disk_postfix']
            nvme_disk_name = "nvme#{port}n#{port_plus}.#{nvme_disk_postfix}"
            nvme_disk = nvme_path + "/" + "#{nvme_disk_name}"
            unless File.exist? (nvme_disk)
	      if provider == "libvirt"
	        cmd = "qemu-img create -f {{ libvirt_nvme_drive_format}}  #{nvme_disk} #{size}M"
	        ok = system(cmd)
	        if ! ok
                  puts "Command failed: #{cmd}"
                  exit
                end
              end
            end
{% if lacks_drive_virtio %}
            non_nvme_drive_interface = "ide"
{% else %}
            non_nvme_drive_interface = "virtio"
{% endif %}
{% if lacks_nvme_support %}
            libvirt.qemuargs :value => "-drive"
            libvirt.qemuargs :value => "file=#{nvme_disk},if=#{non_nvme_drive_interface}"
{% else %}
{% if not libvirt_override_machine_type %}
            bus_for_nvme = "custom-pci-for-nvme"
            pci_function = 0 + port
{% else %}
	    # A PCI-E root port must be added per NVMe device.
            # Chassis must be something unique for the entire topology it seems (?)
            chassis = 50 + port
            bus_for_nvme = "pcie-port-for-nvme-#{port}"
            libvirt.qemuargs :value => "-device"
            libvirt.qemuargs :value => "pcie-root-port,id=#{bus_for_nvme},multifunction=on,bus=pcie.1,addr=0x#{port},chassis=#{chassis}"
            pci_function = "0x0"
{% endif %}

            if zoned
              libvirt.qemuargs :value => "-drive"
              libvirt.qemuargs :value => "file=#{nvme_disk},if=none,id=#{disk_id}"
              libvirt.qemuargs :value => "-device"
              libvirt.qemuargs :value => "nvme,id=nvme#{port},serial=#{serial_id},bus=#{bus_for_nvme},addr=#{pci_function},zoned.zasl=#{zone_zasl}"
              libvirt.qemuargs :value => "-device"
              libvirt.qemuargs :value => "nvme-ns,drive=#{disk_id},bus=nvme#{port},nsid=1,logical_block_size=#{logical_block_size},physical_block_size=#{physical_block_size},zoned=true,zoned.zone_size=#{zone_size},zoned.zone_capacity=#{zone_capacity},zoned.max_open=#{zone_max_open},zoned.max_active=#{zone_max_active}"
            else
              libvirt.qemuargs :value => "-drive"
              libvirt.qemuargs :value => "file=#{nvme_disk},if=none,id=#{disk_id}"
              libvirt.qemuargs :value => "-device"
              libvirt.qemuargs :value => "nvme,id=nvme#{port},serial=#{serial_id},bus=#{bus_for_nvme},addr=#{pci_function}"
              libvirt.qemuargs :value => "-device"
              libvirt.qemuargs :value => "nvme-ns,drive=#{disk_id},bus=nvme#{port},nsid=1"
            end
            port += 1
{% endif %}
	  end
	  if provider == "libvirt"
	    cmd = "sudo chgrp -R #{qemu_group} #{node_custom_data_path}"
	    ok = system("#{cmd}")
	    if ! ok
              puts "Command failed: #{cmd}"
              exit
            end
	    cmd = "sudo chmod -R g+rw #{node_custom_data_path}"
	    ok = system("#{cmd}")
	    if ! ok
              puts "Command failed: #{cmd}"
              exit
            end
	  end # end of provider check for libvirt
	end # end of check for nvme disks for libvirt
{% if bootlinux_9p %}

        libvirt.qemuargs :value => "-device"
        libvirt.qemuargs :value => "{{ bootlinux_9p_driver }},fsdev={{ bootlinux_9p_fsdev }},mount_tag={{ bootlinux_9p_mount_tag }},bus=pcie.0,addr=0x10"
        libvirt.qemuargs :value => "-fsdev"
        libvirt.qemuargs :value => "local,id={{ bootlinux_9p_fsdev }},path={{ bootlinux_9p_host_path }},security_model={{ bootlinux_9p_security_model }}"

{% endif %}
      end # end of libvirt provider code
{% endif %}
    end # end of srv defined loop
  end # end of vagrant_boxes loop
end
