if VAGRANT

choice
	prompt "Vagrant virtualization technology to use"
	default VAGRANT_LIBVIRT

config VAGRANT_LIBVIRT
	bool "Libvirt"
	help
	  Select this option if you want to use KVM / libvirt for
	  local virtualization.

config VAGRANT_VIRTUALBOX
	bool "Virtualbox"
	help
	  Select this option if you want to use Virtualbox for
	  local virtualization.

endchoice

config USE_VAGRANT_LIBVIRT_MIRROR
	bool
	default y if USE_LOCAL_LINUX_MIRROR
	default n if !USE_LOCAL_LINUX_MIRROR

choice
	prompt "Guest vCPUs"
	default VAGRANT_VCPUS_8

config VAGRANT_VCPUS_2
	bool "2"
	help
	  Use 2 vCPUs on guests.

config VAGRANT_VCPUS_4
	bool "4"
	help
	  Use 4 vCPUs on guests.

config VAGRANT_VCPUS_8
	bool "8"
	help
	  Use 8 vCPUs on guests.

config VAGRANT_VCPUS_16
	bool "16"
	help
	  Use 16 vCPUs on guests.

config VAGRANT_VCPUS_32
	bool "32"
	help
	  Use 32 vCPUs on guests.

endchoice

config VAGRANT_VCPUS_COUNT
	int "Number of guest vCPUs"
	default 2 if VAGRANT_VCPUS_2
	default 4 if VAGRANT_VCPUS_4
	default 8 if VAGRANT_VCPUS_8
	default 16 if VAGRANT_VCPUS_16
	default 32 if VAGRANT_VCPUS_32
	help
	  The number of virtual CPUs to use per guest.

choice
	prompt "How much GiB memory to use per guest"
	default VAGRANT_MEM_4G

config VAGRANT_MEM_2G
	bool "2"
	help
	  Use 2 GiB of RAM on guests. Most workflows should work well
	  except for:
	  - xfs/074 is known to fail due to the amount of RAM used by the
	    obsolete xfs_scratch. Since xfs_scratch is obsolete this test
	    will not run by default unless the test runner uses
	    FORCE_XFS_CHECK_PROG=yes
	  - git cloning linux-next requires > 2 GiB RAM if your clone is not
	    shallow (BOOTLINUX_SHALLOW_CLONE)
	  - xfs/084 and generic/627 often get killed by OOM killer and skipped

config VAGRANT_MEM_3G
	bool "3"
	help
	  Use 3 GiB of RAM on guests. No OOM killed tests observed when using
	  3 GiB of RAM on most workflows.

config VAGRANT_MEM_4G
	bool "4"
	help
	  Use 4 GiB of RAM on guests. No known issues are known when using
	  4 GiB of RAM on most workflows.

config VAGRANT_MEM_8G
	bool "8"
	help
	  Use 8 GiB of RAM on guests.

config VAGRANT_MEM_16G
	bool "16"
	help
	  Use 16 GiB of RAM on guests.

config VAGRANT_MEM_32G
	bool "32"
	help
	  Use 32 GiB of RAM on guests.

endchoice

config VAGRANT_MEM_MB
	int "How much RAM to use per guest in MB"
	default 2048 if VAGRANT_MEM_2G
	default 3072 if VAGRANT_MEM_3G
	default 4096 if VAGRANT_MEM_4G
	default 8192 if VAGRANT_MEM_8G
	default 16384 if VAGRANT_MEM_16G
	default 32768 if VAGRANT_MEM_32G
	help
	  How much MiB of RAM to use per guest.

if VAGRANT_LIBVIRT

config HAVE_LIBVIRT_PCIE_PASSTHROUGH
	bool
	default $(shell, scripts/check_pciepassthrough_kconfig.sh passthrough_libvirt.generated)

if HAVE_LIBVIRT_PCIE_PASSTHROUGH
source "vagrant/Kconfig.pcie_passthrough_libvirt"
endif # HAVE_LIBVIRT_PCIE_PASSTHROUGH

choice
	prompt "Machine type to use"
	default LIBVIRT_MACHINE_TYPE_Q35

config LIBVIRT_MACHINE_TYPE_DEFAULT
	bool "Use the default machine type"
	help
	  Use whatever default the guest was intended to use. This will either
	  be the machine type used at virt-install time or some other default
	  by qemu / libvirt. This is important for backward compatibility with
	  older kernels. For example if you enable q35 on an old kernel the old
	  kernel may not boot. For details refer to kdevops commit 83952e2e532e
	  ("vagrant/kdevops_nodes.yaml.in: remove machine_type)".

	  It would seem today's default for libvirt is to still use "pc" which
	  is the old i440x, so you will not get PCIe support.

config LIBVIRT_MACHINE_TYPE_Q35
	bool "q35"
	help
	  Use q35 for the machine type. This will be required for things like
	  CXL or PCIe passthrough.

endchoice

config LIBVIRT_HOST_PASSTHROUGH
	bool "Use CPU host-passthrough"
	default y
	help
	  Enable this to be able to also carry the same CPU your host has to
	  the guest. As per qemu documentation this is the recommended CPU
	  type to use, provided live migration is not required. And we're
	  not supporting live-migration on kdevops so this our default too.
	  This will also enable you to use things like perf stat and
	  get access to some PMUs:

	  perf stat --repeat 2 -e \
	    dTLB-loads,dTLB-load-misses,dTLB-stores,dTLB-stores-misses,\
	    iTLB-loads,iTLB-load-misses,\
	    itlb_misses.walk_completed_4k\
	    itlb_misses.walk_completed_2m_4m\
	    page-faults,tlb:tlb_flush  \
	      --pre 'make -s mrproper defconfig' \
	    \-- make -s -j$(nproc) bzImage

config QEMU_BUILD
	bool "Should we build qemu for you?"
	select NEEDS_LOCAL_DEVELOPMENT_PATH
	help
	  You only want to enable this option if your distribution package
	  of qemu does not have support for the features you need. For
	  example this may be useful if you are a qemu developer or are
	  relying on technology is still under development or if you have
	  a custom qemu git URL.

if !QEMU_BUILD

config QEMU_USE_DEVELOPMENT_VERSION
	bool "Should we look for a development version of qemu?"
	help
	  You want to enable this option if for example the currently
	  available version of qemu does not yet have support for the feature
	  you are going to be working on.

	  Say yes here if you are compiling your own version of qemu.

endif # !QEMU_BUILD

if QEMU_BUILD

choice
	prompt "Qemu git URL to use"
	default QEMU_BUILD_UPSTREAM

config QEMU_BUILD_UPSTREAM
	bool "https://github.com/qemu/qemu.git"
	help
	  Select this option if you want to use the upstream qemu git repo.

config QEMU_BUILD_JIC23
	bool "https://gitlab.com/jic23/qemu.git"
	help
	  Select this option if you want to use Cameron's qemu git repo.
	  This has a few CXL bells and whistles which are not yet upstream.

config QEMU_BUILD_MANUAL
	bool "Custom qemu git URL"
	help
	  Select this option if you want to specify your own git URL.

endchoice

config QEMU_BUILD_GIT
	string "Git tree for qemu to clone on localhost"
	default "/mirror/qemu.git" if USE_LOCAL_LINUX_MIRROR && QEMU_BUILD_UPSTREAM || QEMU_BUILD_MANUAL
	default "https://github.com/qemu/qemu.git" if !USE_LOCAL_LINUX_MIRROR && QEMU_BUILD_UPSTREAM || QEMU_BUILD_MANUAL
	default "https://gitlab.com/jic23/qemu.git" if !USE_LOCAL_LINUX_MIRROR && QEMU_BUILD_JIC23
	help
	  This is the git URL to use to clone and then build qemu for you on
	  your localhost.

config QEMU_BUILD_GIT_DATA_PATH
	string "The destination directory where to clone the qemu git tree"
	default "{{local_dev_path}}/qemu"
	help
	  This is the target location of where to clone the above git tree.
	  Note that {{local_dev_path}} corresponds to the location set by the
	  configuration option CONFIG_NEEDS_LOCAL_DEVELOPMENT_PATH.

config QEMU_BUILD_GIT_VERSION
	string "The version of qemu to build"
	default "v7.2.0"
	help
	  This is the target build version of qemu to build. Please use
	  at least v7.2.0 for CXL support.

config QEMU_USE_DEVELOPMENT_VERSION
	bool
	default y

endif # QEMU_BUILD

config QEMU_BIN_PATH_LIBVIRT
	string "Qemu binary path to use"
	default "/usr/local/bin" if QEMU_USE_DEVELOPMENT_VERSION
	default "/usr/bin" if !QEMU_USE_DEVELOPMENT_VERSION

config QEMU_VIRSH_CAN_SUDO
	bool
	default $(shell, ./scripts/get_libvirsh_can_sudo.sh)

choice
	prompt "Libvirt extra storage driver to use"
	default LIBVIRT_EXTRA_STORAGE_DRIVE_NVME

config LIBVIRT_EXTRA_STORAGE_DRIVE_NVME
	bool "NVMe"
	help
	  Use the qemu nvme driver for extra storage drives. We always expect
	  to use this as we expect *could* be outperforming the virtio driver.
	  Only if you enable this will you get support for ZNS too. We expect
	  the nvme driver to always be the default. It also gives us parity with
	  cloud bringups.

config LIBVIRT_EXTRA_STORAGE_DRIVE_VIRTIO
	bool "virtio"
	help
	  Use the qemu virtio driver for extra storage drives. Use this if you
	  are having issues with "nvme timeouts" issues when testing in a loop
	  with fstests and cannot upgrade your qemu version. If you select this
	  you won't be able to test ZNS.

config LIBVIRT_EXTRA_STORAGE_DRIVE_IDE
	bool "ide"
	help
	  Use the qemu ide driver for extra storage drives. This is useful for
	  really old Linux distributions that lack the virtio backend driver.

endchoice

if LIBVIRT_EXTRA_STORAGE_DRIVE_VIRTIO

choice
	prompt "Libvirt virtio aio"
	default LIBVIRT_VIRTIO_AIO_MODE_NATIVE

config LIBVIRT_VIRTIO_AIO_MODE_NATIVE
	bool "aio=native"
	help
	  Use the aio=native mode. For some older kernels it is known that
	  native will cause corruption if used on ext4 or xfs filesystem if
	  you also use cace=none. This corruption is documented for RHEL:

	  https://access.redhat.com/articles/41313
	  https://bugzilla.redhat.com/show_bug.cgi?id=615309

	  In terms of performance there are some recommendations to not use
	  aio=native on sparsefiles. The claim seems to be that a host
	  filesystems metadata write can block the AIO io_submit() call and
	  therefore block qemu threads which expect AIO behaviour on the guest.
	  This is documented when on Openstack nova an aio mode option was
	  requested to be available for different backends:

	  https://review.opendev.org/c/openstack/nova-specs/+/232514/7/specs/mitaka/approved/libvirt-aio-mode.rst

config LIBVIRT_VIRTIO_AIO_MODE_THREADS
	bool "aio=threads"
	help
	  Use the aio=threads mode. This might be more suitable for you if on
	  older kernels such as in RHEL6 and using sparsefiles and ext4 or xfs
	  on this host with cache=none.

endchoice

config LIBVIRT_VIRTIO_AIO_MODE
	string
	default "native" if LIBVIRT_VIRTIO_AIO_MODE_NATIVE
	default "threads" if LIBVIRT_VIRTIO_AIO_MODE_THREADS

choice
	prompt "Libvirt cache mode"
	default LIBVIRT_VIRTIO_AIO_CACHE_MODE_NONE

config LIBVIRT_VIRTIO_AIO_CACHE_MODE_NONE
	bool "cache=none"
	help
	  Use the cache=none. IO from the guest is not cached on the host but
	  it may be kept in a writeback disk cache. This means that the actual
	  storage device may report a write as completed when the data is still
	  placed in the host's write queue only, the guest's virtual storage
	  adapter is informed that there is a writeback cache. So in essence
	  the guest's writes are directly accessing the host's disk. Use this
	  option for guests with large IO requirements. This is generally the
	  best option and is required for live migration.

config LIBVIRT_VIRTIO_AIO_CACHE_MODE_WRITETHROUGH
	bool "cache=writethrough"
	help
	  cache=writethrough. IO from the guest is cached on the host but
	  written through to the physical medium. Writes are only reported as
	  completed when the data has been committed to the storage device. The
	  guest's virtual storage adapter is informed that there is no
	  writeback cache so the guest does not need to send flush commands
	  to manage integrity. This mode is slower and prone to scaling issues.
	  Best used for small number of guests with lower IO reqs. This should
	  be used on older guests which do not support writeback cache.

config LIBVIRT_VIRTIO_AIO_CACHE_MODE_WRITEBACK
	bool "cache=writeback"
	help
	  cache=writeback. IO from the guest is cached on the host so it is
	  cached on the host's page cache. Writes are reported to the guest
	  when they are placed on the host's page cache. The guest's virtual
	  storage controller is informed of the writeback cache and therefore
	  expected to send flush commands as needed to manage data integrity.

config LIBVIRT_VIRTIO_AIO_CACHE_MODE_DIRECTSYNC
	bool "cache=directsync"
	help
	  cache=directsync. Writes are reported only when the data has been
	  committed to the storage controller. The host cache is completely
	  bypassed. This mode is useful for guests which that do not send
	  flushes when needed.

config LIBVIRT_VIRTIO_AIO_CACHE_MODE_UNSAFE
	bool "cache=unsafe"
	help
	  cache=unsafe. Similar to writeback except all of the flush commands
	  from the guest are ignored. This is useful if the one wants to
	  prioritize performance and one does not care about data loss.

endchoice

config LIBVIRT_VIRTIO_AIO_CACHE_MODE
	string
	default "none" if LIBVIRT_VIRTIO_AIO_CACHE_MODE_NONE
	default "writethrough" if LIBVIRT_VIRTIO_AIO_CACHE_MODE_WRITETHROUGH
	default "writeback" if LIBVIRT_VIRTIO_AIO_CACHE_MODE_WRITEBACK
	default "directsync" if LIBVIRT_VIRTIO_AIO_CACHE_MODE_DIRECTSYNC
	default "unsafe" if LIBVIRT_VIRTIO_AIO_CACHE_MODE_UNSAFE

endif

choice
	prompt "Libvirt storage pool path"
	default LIBVIRT_STORAGE_POOL_PATH_INFER_ADVANCED if DISTRO_DEBIAN && QEMU_VIRSH_CAN_SUDO
	default LIBVIRT_STORAGE_POOL_PATH_CUSTOM_DEFAULT_DISTRO if !DISTRO_SUSE && !DISTRO_DEBIAN
	default LIBVIRT_STORAGE_POOL_PATH_CUSTOM_CWD if DISTRO_SUSE

config LIBVIRT_STORAGE_POOL_PATH_INFER_ADVANCED
	bool "Use an advanced smart inference for what storage pool path to use"
	help
	  If you are a power user of kdevops you likely want to enable this.
	  By default vagrant will assume that if you don't have a virsh pool
	  that the current directory will become the "default" storage pool
	  path. This is rather silly for an advanced setup. Consider a setup
	  where you have a set of different nvme drivers mounted on different
	  partitions:

	  /dev/nvme0n1 --> /data1 with xfs
	  /dev/nvme1n1 --> /data2 with btrfs
	  /dev/nvme2n1 --> /data2 with ext4

	  In this setup, if you have a kdevops instance initialized in
	  /data1/ somewhere you likely want to use a virsh pool path under
	  /data1/libvirt/images/ so that all local requests to that drive
	  go to that storage pool that. By enabling this option kdevops will
	  infer this information for you and figure out what storage pool path
	  to use. It will scrape your existing virsh pool-list and use the first
	  path where the first directory of your current working directory
	  lies.

	  This is enabled by default on Debian only now as this has been
	  tested there first. If this gets tested on other distributions
	  they should migrate over to enable power users to be more lazy
	  on initial bringups.

	  Eventually this can be the default but it requires high confidence
	  that the heuristics will work on each distro with their own defaults.

config LIBVIRT_STORAGE_POOL_PATH_CUSTOM_CWD
	bool "Use the current vagrant working directory"
	help
	  Select this option if you want to use the vagrant directory inside
	  where you git cloned kdevops as the libvirt storage pool path where
	  we'll download images and store images for guests spawned. If users
	  git cloned kdevops somewhere in their home directory they'll have to
	  make sure that the group which libvirt is configured to run for their
	  distribution can have access to that directory. As it is today it
	  would seem only fedora restricts the $HOME to g-rwx o-rwx and so
	  by default this option won't work on Fedora by default. This used
	  to be the old default on kdevops but because of this it should not
	  be used by default. Distributions still relying on this should
	  verify they can change this default.

config LIBVIRT_STORAGE_POOL_PATH_CUSTOM_DEFAULT_DISTRO
	bool "Use the same path typically used by each distribution"
	help
	  Select this option if you want to use the same location as the
	  distribution would typically use. We expect this to be
	  /var/lib/libvirt/images/ for most distributions, however we can
	  customize this further if this is not true by adding further checks.

config LIBVIRT_STORAGE_POOL_PATH_CUSTOM_MANUAL
	bool "Custom manual path"
	help
	  Select this option if you want to manually specify where to use as
	  the qemu storage pool path. This is today's default given otherwise
	  we may have to muck with the $HOME directory permissions.

endchoice

config LIBVIRT_STORAGE_POOL_CREATE
	bool "Should we build a custom storage pool for you?"
	default n if !LIBVIRT_STORAGE_POOL_PATH_INFER_ADVANCED
	default $(shell, ./scripts/get_libvirsh_pool_enabled.sh) if LIBVIRT_STORAGE_POOL_PATH_INFER_ADVANCED
	help
	  By default vagrant assumes your storage pool name is "default" and
	  it expects libvirt to have created this for you. If you want to
	  use a custom pool name and path enable this. This is useful if
	  you want to place guest images on another path other than the
	  default libvirt has setup for you on the "default" pool name.

config LIBVIRT_STORAGE_POOL_NAME
	string "Libvirt storage pool name"
	depends on LIBVIRT_STORAGE_POOL_CREATE
	default "default" if !LIBVIRT_STORAGE_POOL_PATH_INFER_ADVANCED
	default $(shell, ./scripts/get_libvirsh_pool_name.sh) if LIBVIRT_STORAGE_POOL_PATH_INFER_ADVANCED
	help
	  The libvirt storage pool name to use. By default this is "default",
	  and this is typically defined by libvirt. Even if you create guests
	  on a separate directory unless you use a custom storage pool name
	  here the default path used where libvirt created the default
	  storage pool path will be used for data. You should modify the
	  storage pool name to something other than "default" here if you
	  are using a custom storage pool path and are doing so to ensure
	  you don't waste space in whatever path libvirt's default storage
	  pool path is set to. To check your libvirt's default storage pool
	  path you can run this and look for the path:

	    virsh pool-dumpxml default

	  You will need to use sudo on all distros which do not use
	  LIBVIRT_URI_SESSION (so all distros other than Fedora).
	  If you set here something other than 'default' kdevops will create
	  this pool upon 'make bringup' if the pool is not yet available. It
	  will do this with:

	    virsh pool-define-as NAME PATH
	    virsh pool-start NAME
	    virsh pool-autostart NAME

	  That name you use here is up to you but you should use something
	  which would make it obvious what it is for other users on the system.
	  For instance you may want to use a volume name of "data2" for a path
	  on a partition on /data2/ or something like that.

config LIBVIRT_STORAGE_POOL_PATH_CUSTOM
	string "Libvirt storage pool path"
	default $(shell, ./scripts/get_libvirsh_pool_path.sh) if LIBVIRT_STORAGE_POOL_PATH_INFER_ADVANCED
	default "/var/lib/libvirt/images/" if LIBVIRT_STORAGE_POOL_PATH_CUSTOM_DEFAULT_DISTRO
	default $(shell, scripts/cwd-append.sh vagrant) if LIBVIRT_STORAGE_POOL_PATH_CUSTOM_CWD
	default "/opt/libvirt/images/" if LIBVIRT_STORAGE_POOL_PATH_CUSTOM_MANUAL
	help
	  The path to use for the libvirt storage pool path. Since kdevops uses
	  vagrant for virtualization this is also the path used to place the
	  additional nvme drives created. kdevops adds a postfix "kdevops" to
	  this directory as it wants to allow vagrant full control over that
	  directory. For instance if this is /var/lib/libvirt/images/ kdevops
	  will let vagrant store images in /var/lib/libvirt/images/ and
	  the nvme qcow2 files created will go in by default to the directory
	  /var/lib/libvirt/images/kdevops/.vagrant/nvme_disks/guest-hostname/.

choice
	prompt "Libvirt URI"
	default LIBVIRT_URI_SYSTEM if !DISTRO_FEDORA
	default LIBVIRT_URI_SESSION if DISTRO_FEDORA

config LIBVIRT_URI_SYSTEM
	bool "Use qemu:///system for the URI"
	help
	  The first design behind libvirt is to use the system URI, that is,
	  qemu:///system. All 'system' URIs (be it qemu, lxc, uml, ...)
	  connect to the libvirtd daemon running as root which is launched at
	  system startup. Virtual machines created and run using 'system'
	  are usually launched as root, unless configured otherwise (for
	  example in /etc/libvirt/qemu.conf). A distribution can however still
	  allow users to use the system URI if they are added to the respective
	  groups to use libvirt, and this is the approach taken by kdevops when
	  this option is enabled.

	  You will definitely want to use qemu:///system if your VMs are
	  acting as servers. VM autostart on host boot only works for 'system',
	  and the root libvirtd instance has necessary permissions to use
	  proper networkings via bridges or virtual networks. qemu:///system
	  is generally what tools like virt-manager default to.

	  When this option is enabled vagrant's libvirt default built-in
	  URI is used along with the default network management interface,
	  libvirt socket, and the network interface assumed for bridging.

	  For more details on this refer to the libvirt wiki which still
	  advises in favor of the system URI over the session URI:

	  https://wiki.libvirt.org/page/FAQ#What_is_the_difference_between_qemu:.2F.2F.2Fsystem_and_qemu:.2F.2F.2Fsession.3F_Which_one_should_I_use.3F

config LIBVIRT_URI_SESSION
	bool "Use qemu:///session for the URI"
	help
	  A second design consideration has been implemented into libvirt to
	  enable users to use libvirt without the libvirt daemon needing to
	  run as root. All 'session' URIs launch a libvirtd instance as your
	  local user, and all VMs are run with local user permissions.

	  The benefit of qemu:///session is that permission issues vanish:
	  disk images can easily be stored in $HOME, serial PTYs are owned by
	  the user, etc.

	  qemu:///session has a serious drawback: since the libvirtd instance
	  does not have sufficient privileges, the only out of the box network
	  option is qemu's usermode networking, which has nonobvious
	  limitations, so its usage is discouraged. More info on qemu
	  networking options: http://people.gnome.org/~markmc/qemu-networking.html
	  With regards to kdevops, if you use the session URI we don't
	  instantiate secondary interfaces with private IP addresses. This is
	  not a requirement for the currently supported workflows but if
	  you are doing custom networking stuff this may be more relevant for
	  you. Fedora defaults to the session URI.

	  When this option is enabled we modify vagrant's libvirt default
	  built-in URI for the session URI, and we also modify the default
	  network management interface to be virbr0, the default socket
	  is assumed to be /run/libvirt/libvirt-sock-ro. New kconfig options
	  can be added later to customize those further if we really need
	  to.

	  Please note that sensible defaults are enabled for your Linux
	  distribution, so if your distribution does not have session URI
	  set by default it means it doesn't support it yet and you should
	  expect things to not work, and put the work to fix / enhance that
	  somehow. That work likely is not on kdevops... but perhaps this
	  could be worng. Testing has be done with session support on debian
	  testing, Ubuntu 21.10 and they both have issues. Don't enable session
	  support manually unless you know what you are doing.

config LIBVIRT_URI_CUSTOM
	bool "Custom qemu URI"
	help
	  Select this option if you want to manually specify which URI to use.
	  In other words you know what you are doing.

endchoice

config LIBVIRT_URI_PATH
	string "Libvirt qemu URI to use"
	default "qemu:///system" if LIBVIRT_URI_SYSTEM || LIBVIRT_URI_CUSTOM
	default "qemu:///session" if LIBVIRT_URI_SESSION
	help
	  By default vagrant uses a qemu:///system URI which assumes the libvirt
	  daemon runs as a user other than the user which is running the vagrant
	  commands. Libvirt has support for running the libvirt daemon as other
	  users using session support. This will be modified to a session URI
	  if you enable LIBVIRT_URI_SESSION. You can however set this to
	  something different to suit your exact needs here. This is the value
	  passed to the vagrant-libvirt plugin libvirt.uri. You should not have
	  to modify this value if you selected LIBVIRT_URI_SYSTEM or
	  LIBVIRT_URI_SESSION and are starting from a fresh 'make mrproper'
	  setting on kdevops, the appropriate value will be set for you.
	  You should only have to modify this manually if you set
	  LIBVIRT_URI_CUSTOM and you know what you are doing.

config LIBVIRT_SYSTEM_URI_PATH
	string "Libvirt system qemu URI to use"
	default "qemu:///system"
	help
	  This is the URI of QEMU system connection, used to obtain the IP
	  address for management. This is used for the vagrant-libvirt plugin
	  libvirt.system_uri setting. If for whatever reason this needs to
	  be modified you can do so here. Even if you are using session
	  support you should leave this with the default qemu:///system setting
	  as this is still used to ensure your guest's IP address will be
	  communicated back to vagrant so it determines the guest is up and
	  you can ssh to it. Setting this to qemu:///session still gets the
	  guest up but vagrant won't know the guest is up, even though the
	  host can ssh to the guest. You should only modify this value if
	  you know what you are doing.

endif

if VAGRANT_VIRTUALBOX

choice
	prompt "Virtualbox storage pool path"
	default VIRTUALBOX_STORAGE_POOL_PATH_CUSTOM_MANUAL if !DISTRO_SUSE
	default VIRTUALBOX_STORAGE_POOL_PATH_CUSTOM_CWD if DISTRO_SUSE

config VIRTUALBOX_STORAGE_POOL_PATH_CUSTOM_CWD
	bool "Use the current vagrant working directory"
	help
	  Select this option if you want to use the vagrant directory inside
	  where you git cloned kdevops as the vagrant storage pool path where
	  additional nvme drives will be created. As it is today it
	  would seem only fedora restricts the $HOME to g-rwx o-rwx and so
	  by default this option won't work on Fedora by default. This used
	  to be the old default on kdevops but because of this it should not
	  be used by default. Distributions still relying on this should
	  verify they can change this default.

config VIRTUALBOX_STORAGE_POOL_PATH_CUSTOM_MANUAL
	bool "Custom manual path"
	help
	  Select this option if you want to manually specify where to use as
	  the directory where we'll have kdevops create additional nvme drives
	  for virtualbox to use. This is today's default given otherwise
	  we may have to muck with the $HOME directory permissions.

endchoice

config VIRTUALBOX_STORAGE_POOL_PATH_CUSTOM
	string "Virtualbox storage pool path"
	default $(shell, scripts/cwd-append.sh vagrant) if VIRTUALBOX_STORAGE_POOL_PATH_CUSTOM_CWD
	default "/opt/virtualbox/storage/" if VIRTUALBOX_STORAGE_POOL_PATH_CUSTOM_MANUAL
	help
	  The path to use for creating additional nvme drives used by
	  virtualbox. kdevops adds a postfix "kdevops" to this directory as it
	  wants to allow vagrant full control over that directory. For instance
	  if this is /opt/virtualbox/storage/ kdevops will have virtualbox
	  create the nvme files under the directory
	  /opt/virtualbox/storage/kdevops/.vagrant/nvme_disks/guest-hostname/.

choice
	prompt "Virtualbox extra drive format"
	default VIRTUALBOX_EXTRA_DRIVE_VDI

config VIRTUALBOX_EXTRA_DRIVE_VDI
	bool "vdi"
	help
	  Select this option if you want to use the vdi format.

config VIRTUALBOX_EXTRA_DRIVE_VMDK
	bool "vmdk"
	help
	  Select this option if you want to use the vmdk format.

config VIRTUALBOX_EXTRA_DRIVE_VHD
	bool "vhd"
	help
	  Select this option if you want to use the vhd format.

endchoice

config VIRTUALBOX_EXTRA_DRIVE_FORMAT
	string
	default "vdi" if VIRTUALBOX_EXTRA_DRIVE_VDI
	default "vmdk" if VIRTUALBOX_EXTRA_DRIVE_VMDK
	default "vhd" if VIRTUALBOX_EXTRA_DRIVE_VHD

endif

config LIBVIRT_QEMU_GROUP
	string
	default "qemu" if !DISTRO_DEBIAN && !DISTRO_UBUNTU
	default "libvirt-qemu" if DISTRO_DEBIAN || DISTRO_UBUNTU

config KDEVOPS_STORAGE_POOL_PATH
	string
	default LIBVIRT_STORAGE_POOL_PATH_CUSTOM if VAGRANT_LIBVIRT
	default VIRTUALBOX_STORAGE_POOL_PATH_CUSTOM if VAGRANT_VIRTUALBOX

config QEMU_BIN_PATH
	string
	default QEMU_BIN_PATH_LIBVIRT if VAGRANT_LIBVIRT
	default "/usr/bin" if !VAGRANT_LIBVIRT

config LIBVIRT_URI
	string
	default "qemu:///system" if !VAGRANT_LIBVIRT
	default LIBVIRT_URI_PATH if VAGRANT_LIBVIRT

config LIBVIRT_SYSTEM_URI
	string
	default "qemu:///system" if !VAGRANT_LIBVIRT
	default LIBVIRT_SYSTEM_URI_PATH if VAGRANT_LIBVIRT

config LIBVIRT_SESSION
	bool
	default LIBVIRT_URI_SESSION

# Only fedora is using this for now. We can add options to modify
# this once this changes or if someone wants to really modify these.
if LIBVIRT_SESSION

config LIBVIRT_SESSION_SOCKET
	string
	default "/run/libvirt/libvirt-sock-ro"

config LIBVIRT_SESSION_MANAGEMENT_NETWORK_DEVICE
	string
	default "virbr0"

config LIBVIRT_SESSION_PUBLIC_NETWORK_DEV
	string
	default "virbr0"

endif # LIBVIRT_SESSION

config HAVE_SUSE_VAGRANT
	bool
	default $(shell, scripts/check_distro_kconfig.sh suse)

choice
	prompt "Vagrant Guest Linux distribution to use"
	default VAGRANT_DEBIAN if !HAVE_SUSE_VAGRANT
	default VAGRANT_SUSE if HAVE_SUSE_VAGRANT

config VAGRANT_DEBIAN
	bool "Debian"
	select HAVE_CUSTOM_DISTRO_HOST_PREFIX
	select HAVE_DISTRO_XFS_PREFERS_MANUAL if FSTESTS_XFS
	select HAVE_DISTRO_BTRFS_PREFERS_MANUAL if FSTESTS_BTRFS
	select HAVE_DISTRO_EXT4_PREFERS_MANUAL if FSTESTS_EXT4
	select HAVE_DISTRO_PREFERS_CUSTOM_HOST_PREFIX
	select HAVE_DISTRO_PREFERS_FSTESTS_WATCHDOG if KDEVOPS_WORKFLOW_ENABLE_FSTESTS
	select HAVE_DISTRO_PREFERS_FSTESTS_WATCHDOG_KILL if KDEVOPS_WORKFLOW_ENABLE_FSTESTS
	help
	  This option will set the target guest to Debian.

config VAGRANT_OPENSUSE
	bool "OpenSUSE"
	select HAVE_CUSTOM_DISTRO_HOST_PREFIX
	select HAVE_DISTRO_XFS_PREFERS_MANUAL if FSTESTS_XFS
	select HAVE_DISTRO_BTRFS_PREFERS_MANUAL if FSTESTS_BTRFS
	select HAVE_DISTRO_EXT4_PREFERS_MANUAL if FSTESTS_EXT4
	select HAVE_DISTRO_BLKTESTS_PREFERS_MANUAL if KDEVOPS_WORKFLOW_ENABLE_BLKTESTS
	select HAVE_DISTRO_PREFERS_CUSTOM_HOST_PREFIX
	select HAVE_DISTRO_PREFERS_FSTESTS_WATCHDOG if KDEVOPS_WORKFLOW_ENABLE_FSTESTS
	select HAVE_DISTRO_PREFERS_FSTESTS_WATCHDOG_KILL if KDEVOPS_WORKFLOW_ENABLE_FSTESTS
	help
	  This option will set the target guest to OpenSUSE.

config VAGRANT_SUSE
	bool "SUSE"
	depends on HAVE_SUSE_VAGRANT
	select HAVE_KDEVOPS_CUSTOM_DEFAULTS
	select HAVE_CUSTOM_DISTRO_HOST_PREFIX
	select HAVE_DISTRO_PREFERS_CUSTOM_HOST_PREFIX
	select HAVE_DISTRO_XFS_PREFERS_MANUAL if FSTESTS_XFS
	select HAVE_DISTRO_BTRFS_PREFERS_MANUAL if FSTESTS_BTRFS
	select HAVE_DISTRO_BLKTESTS_PREFERS_MANUAL if KDEVOPS_WORKFLOW_ENABLE_BLKTESTS
	select HAVE_DISTRO_SUSE
	select HAVE_DISTRO_PREFERS_REGISTRATION
	select HAVE_DISTRO_REG_METHOD_TWOLINE
	select VAGRANT_INSTALL_PRIVATE_BOXES
	select HAVE_CUSTOM_KDEVOPS_GIT
	select HAVE_CUSTOM_KDEVOPS_GIT_DATA
	select HAVE_CUSTOM_KDEVOPS_DIR
	help
	  This option will set the target guest to SUSE. There is currently
	  no scriptable way to download vagrant images, however the images
	  are available for download via:

	    https://suse.com/download

config VAGRANT_FEDORA
	bool "Fedora"
	select HAVE_DISTRO_XFS_PREFERS_MANUAL if FSTESTS_XFS
	select HAVE_DISTRO_BTRFS_PREFERS_MANUAL if FSTESTS_BTRFS
	select HAVE_DISTRO_EXT4_PREFERS_MANUAL if FSTESTS_EXT4
	select HAVE_DISTRO_PREFERS_FSTESTS_WATCHDOG if KDEVOPS_WORKFLOW_ENABLE_FSTESTS
	select HAVE_DISTRO_PREFERS_FSTESTS_WATCHDOG_KILL if KDEVOPS_WORKFLOW_ENABLE_FSTESTS
	help
	  This option will set the target guest to Fedora.

config VAGRANT_REDHAT_GENERIC
	bool "Redhat generic"
	select HAVE_CUSTOM_DISTRO_HOST_PREFIX
	select HAVE_DISTRO_PREFERS_CUSTOM_HOST_PREFIX
	help
	  This option will set the target guest to Redhat using the generic
	  vagrant source as the author. In other words, you are on your own
	  with these. And you won't be able to do much with these guests
	  other than get a base guest up. Patches are welcomed to enable
	  registration to make them more useful.

config VAGRANT_KDEVOPS
	bool "kdevops kernel builds"
	select HAVE_CUSTOM_DISTRO_HOST_PREFIX
	select HAVE_DISTRO_XFS_PREFERS_MANUAL if FSTESTS_XFS
	select HAVE_DISTRO_BTRFS_PREFERS_MANUAL if FSTESTS_BTRFS
	select HAVE_DISTRO_EXT4_PREFERS_MANUAL if FSTESTS_EXT4
	select HAVE_DISTRO_PREFERS_FSTESTS_WATCHDOG if KDEVOPS_WORKFLOW_ENABLE_FSTESTS
	select HAVE_DISTRO_PREFERS_FSTESTS_WATCHDOG_KILL if KDEVOPS_WORKFLOW_ENABLE_FSTESTS
	help
	  This option will let you select custom kernel builds by the
	  kdevops project. The distributions may vary and are are specified.

endchoice

config HAVE_VAGRANT_BOX_VERSION
	bool
	default n

source "vagrant/Kconfig.debian"
source "vagrant/Kconfig.opensuse"
source "vagrant/Kconfig.fedora"
source "vagrant/Kconfig.redhat.generic"
source "vagrant/Kconfig.kdevops"

if HAVE_SUSE_VAGRANT
source "vagrant/Kconfig.suse"
endif # HAVE_SUSE_VAGRANT

config VAGRANT_BOX
	string "Vagrant box"
	default "debian/testing64" if VAGRANT_DEBIAN_TESTING64 || !VAGRANT
	default "debian/bullseye64" if VAGRANT_DEBIAN_BULLSEYE64
	default "debian/buster64" if VAGRANT_DEBIAN_BUSTER64
	default "opensuse/Tumbleweed.x86_64" if VAGRANT_OPENSUSE_X86_64_TW
	default "fedora/37-cloud-base" if VAGRANT_FEDORA_X86_64_37_CLOUD_BASE
	default "generic/rhel8" if VAGRANT_REDHAT_GENERIC_RHEL8_X86_64
	default "generic/rhel9" if VAGRANT_REDHAT_GENERIC_RHEL9_X86_64
	default "opensuse/Leap-15.3.x86_64" if VAGRANT_OPENSUSE_X86_64_LEAP_15_3
	default "Leap-15.4.x86_64" if VAGRANT_OPENSUSE_X86_64_LEAP_15_4
	default "linux-kdevops/debian-next-20220629" if VAGRANT_KDEVOPS_DEBIAN_TESTING64_NEXT_20220629 || !VAGRANT
	default VAGRANT_SUSE_BOX if VAGRANT_SUSE
	help
	  The vagrant box to use.

config VAGRANT_BOX_UPDATE_ON_BRINGUP
	bool "Run vagrant box update prior to bringup"
	default y
	help
	  When you use vagrant on a system the target box for the distribution
	  will first be downloaded on its first use. It will not be updated
	  until you manually run "vagrant box update" on the respective vagrant
	  directory. This can mean that if your system first used kdevops in
	  January 2022 and you are using debian-testing, the old version of
	  debian testing as of January 2022 will be used if you try to
	  instantiate a guest in January 2023. This can means your userspace
	  may be too old to deal with some new kernel features. An example
	  is zstd module support for initframfs does not work well with old
	  userspace on debian-testing. The boot will fail when trying to
	  run "make linux" and it would not be clear why, the reason would be
	  a failed ext4 module could not be loaded, in fact no modules could
	  be loaded by the initramfs. To fix the user experience we ensure we
	  always run "vagrant box update" as the first step to "make bringup".

	  Folks testing stable kernels will use stable distributions, such as
	  debian bullseye that don't move userspace as often but in fact welcome
	  their own stable updates to userspace, and this is why this is kept
	  as enabled by default.

	  You may want to disable this if you don't want to deal with these
	  updates on bringup, and you want to do the updates on your own.
	  Note that if you already ran bringup with an old box, you won't
	  reap benefits of the new one until you "make destroy" and then
	  "make bringup" again. If you are already on an old box and don't
	  want to destroy your current box image you can just upgrade userspace
	  manually by the typical distro means to update itself. For example
	  on debian-testing that's "apt-get update && apt-get dist-upgrade".

config VAGRANT_BOX_UPDATE_ON_BRINGUP
	bool "Validate Vagrantfile prior to bringup"
	default y
	help
	  Folks hacking on the Vagrantfile may want to ensure they validate
	  the Vagrantfile first. This will allow developers to identify issues
	  prior to running some real virtualization commands. Enable this by
	  default as validation is fast and safe.

config VAGRANT_PREFERRED_KERNEL_CI_SUBJECT_TOPIC
	string
	default VAGRANT_BOX if VAGRANT_DEBIAN_BUSTER64
	default VAGRANT_BOX if VAGRANT_DEBIAN_BULLSEYE64


config HAVE_VAGRANT_BOX_URL
	bool

if HAVE_VAGRANT_BOX_URL

config VAGRANT_BOX_URL
	string
	depends on HAVE_VAGRANT_BOX_URL
	default VAGRANT_SUSE_BOX_URL if HAVE_SUSE_VAGRANT
	default "https://download.opensuse.org/repositories/Virtualization:/Appliances:/Images:/openSUSE-Leap-15.4/images/boxes/Leap-15.4.x86_64.json" if VAGRANT_OPENSUSE_X86_64_LEAP_15_4

endif # HAVE_VAGRANT_BOX_URL

if HAVE_VAGRANT_BOX_VERSION

config VAGRANT_BOX_VERSION
	string "Vagrant box version"
	default "1.0.20220528" if VAGRANT_OPENSUSE_X86_64_TW_1020220528
	default "1.0.20210915" if VAGRANT_OPENSUSE_X86_64_TW_1020210915
	default "1.0.20210203" if VAGRANT_OPENSUSE_X86_64_TW_1020210203
	default "1.0.20200714" if VAGRANT_OPENSUSE_X86_64_TW_1020200714
	default "1.0.20210203" if VAGRANT_OPENSUSE_X86_64_TW_1020210203
	default "0.1.0" if VAGRANT_KDEVOPS_DEBIAN_TESTING64_NEXT_20220629
	help
	  The vagrant box version to use. This is set for you depending on the
	  image you select. You can manually override the version we have last
	  tested here.

endif # HAVE_VAGRANT_BOX_VERSION

if !HAVE_VAGRANT_BOX_VERSION

config VAGRANT_BOX_VERSION
	string
	default ""

endif # !HAVE_VAGRANT_BOX_VERSION

config VAGRANT_LIBVIRT_INSTALL
	bool "Installs libvirt"
	default y if KDEVOPS_FIRST_RUN
	default n if !KDEVOPS_FIRST_RUN
	help
	  If this option is enabled then the ansible role which installs
	  libvirt for you will be run. The goal will be to ensure you have
	  libvirt installed and running.

config VAGRANT_LIBVIRT_CONFIGURE
	bool "Configure libvirt so you spawn guests as a regular user"
	default y if KDEVOPS_FIRST_RUN
	default n if !KDEVOPS_FIRST_RUN
	help
	  If this option is enabled then the ansible role which configures
	  libvirt for you will be run. This typically just requires adding the
	  user to a specific set of groups. The user must log out and back
	  in again, to ensure the new group takes effect. The goal in the
	  configuration will be to ensure you can use libvirt to spawn guests
	  as a regular user. You are encouraged to say y here unless you know
	  what you are doing or you already know this works. If you are unsure,
	  the litmus test for this is if you can run vagrant up, on any public
	  demo box available.

config VAGRANT_LIBVIRT_VERIFY
	bool "Verify that a user can spawn libvirt as a regular user"
	default y if KDEVOPS_FIRST_RUN
	default n if !KDEVOPS_FIRST_RUN
	help
	  To enable a user to be able to spawn libvirt guests as a regular user
	  a user is typically added to a few groups. These groups are not
	  effective immediately, and so before a user can assume that they
	  use vagrant they must verify that the required groups are effective.
	  If you enable this option, we will spawn an ansible role that will
	  verfify and ensure that your user is already part of these groups.
	  You can safely say yes here.

config VAGRANT_INSTALL_PRIVATE_BOXES
	bool "Install private vagrant boxes"
	default y
	help
	  If this option is enabled then the ansible role which installs
	  additional vagrant boxes will be run. This is useful if for example,
	  you have private vagrant boxes available and you want to use them.
	  You can safely disable this option if you are using only public
	  vagrant boxes. Enabling this option is safe as well, given no
	  private boxes would be defined, and so nothing is done.

choice
	prompt "Libvirt nvme drive file format"
	depends on VAGRANT_LIBVIRT
	default LIBVIRT_NVME_DRIVE_FORMAT_RAW

config LIBVIRT_NVME_DRIVE_FORMAT_QCOW2
	bool "Use qcow2 format"
	help
	  Select this option if you want to use the qcow2 file format for the
	  nvme drives created for you. This is useful if you want to use
	  advanced features on the files such as growing them or freezing
	  them. There may be some odd issues however with very sub-optimal
	  features such as with discard, however these issues are still being
	  investingated.

config LIBVIRT_NVME_DRIVE_FORMAT_RAW
	bool "Use raw format"
	help
	  Select this option if you want to use the raw file format for the nvme
	  drives created for you. One of the sweet spots for using raw and 4KiB
	  block sizes is the advantages of ensuring that when a filesystem
	  issues a punch hole through fallocate (FALLOC_FL_PUNCH_HOLE) write
	  zeroes (REQ_OP_WRITE_ZEROES) results in an actual deallocation of
	  blocks faster. When using qcow2 the default is to use a cluster of
	  64 KiB for blocks and by default blocks will only be marked
	  deallocated if a full cluster is zeroed or discarded. You can
	  fallocate (punch) holes on a filesystem with a smaller block
	  size than the default qcow2 cluster size (say 4 KiB), and so in
	  theory, this may cause undeterministic delays. In practice this may
	  end up in nvme timeouts, but these issues are currently being
	  investigated, and if using the raw format proves to not cause nvme
	  timeouts as observed with fstests punch tests while the backend
	  drive has low space (below 80%) this may become the default for
	  kdevops as it reflects a possible bug in qemu.

endchoice

config QEMU_ENABLE_NVME_ZNS
	bool "Enable Qemu NVMe ZNS drives"
	depends on VAGRANT_LIBVIRT && LIBVIRT_EXTRA_STORAGE_DRIVE_NVME
	default n
	help
	  If this option is enabled then you can enable NVMe ZNS drives on the
	  guests.

config QEMU_CUSTOM_NVME_ZNS
	bool "Customize Qemu NVMe ZNS settings"
	depends on QEMU_ENABLE_NVME_ZNS
	default n
	help
	  If this option is enabled then you will be able to modify the defaults
	  used for the 2 NVMe ZNS drives we create for you. By default we create
	  two NVMe ZNS drives with 100 GiB of total size, each zone being
	  128 MiB, and so you end up with 800 total zones. The zone capacity
	  equals the zone size. The default zone size append limit is also
	  set to 0, which means the zone append size limit will equal to the
	  maximum data transfer size (MDTS). The default logical and physical
	  block size of 4096 bytes is also used. If you want to customize any
	  of these ZNS settings for the drives we bring up enable this option.

	  If unsure say N.

if QEMU_CUSTOM_NVME_ZNS

config QEMU_CUSTOM_NVME_ZONE_DRIVE_SIZE
	int "Qemu ZNS storage nvme drive size"
	default 102400
	help
	  The size of the qemu nvme ZNS drive to expose. We expose 2 NVMe
	  ZNS drives of 100 GiB by default. This value chagnes its size.
	  100 GiB is a sensible default given most full fstests require about
	  50 GiB of data writes.

config QEMU_CUSTOM_NVME_ZONE_ZASL
	int "Qemu ZNS zasl - zone append size limit power of 2"
	default 0
	help
	  This is the zone append size limit. If left at 0 qemu will use
	  the maximum data transfer size (MDTS) for the zone size append limit.
	  Otherwise if this value is set to something other than 0, then the
	  zone size append limit will equal to 2 to the power of the value set
	  here multiplied by the minimum memory page size (4096 bytes) but the
	  qemu promises this value cannot exceed the maximum data transfer size.

config QEMU_CUSTOM_NVME_ZONE_SIZE
	string "Qemu ZNS storage nvme zone size"
	default "128M"
	help
	  The size the the qemu nvme ZNS zone size. The number of zones are
	  implied by the driver size / zone size. If there is a remainder
	  technically that should go into another zone with a smaller zone
	  capacity.

config QEMU_CUSTOM_NVME_ZONE_CAPACITY
	string "Qemu ZNS storage nvme zone capacity"
	default "0M"
	help
	  The size to use for the zone capacity. This may be smaller or equal
	  to the zone size. If set to 0 then this will ensure the zone
	  capacity is equal to the zone size.

config QEMU_CUSTOM_NVME_ZONE_MAX_ACTIVE
	int "Qemu ZNS storage nvme zone max active"
	default 0
	help
	  The max numbe of active zones. The default of 0 means all zones
	  can be active at all times.

config QEMU_CUSTOM_NVME_ZONE_MAX_OPEN
	int "Qemu ZNS storage nvme zone max open"
	default 0
	help
	  The max numbe of open zones. The default of 0 means all zones
	  can be opened at all times. If the number of active zones is
	  specified this value must be less than or equal to that value.

config QEMU_CUSTOM_NVME_ZONE_PHYSICAL_BLOCKSIZE
	int "Qemu ZNS storage nvme physical block size"
	default 4096
	help
	  The physical block size to use for ZNS drives. This ends up
	  what is put into the /sys/block/<disk>/queue/physical_block_size
	  and is the smallest unit a physical storage device can write
	  atomically. It is usually the same as the logical block size but may
	  be bigger. One example is SATA drives with 4KB sectors that expose a
	  512-byte logical block size to the operating system. For stacked
	  block devices the physical_block_size variable contains the maximum
	  physical_block_size of the component devices.

config QEMU_CUSTOM_NVME_ZONE_LOGICAL_BLOCKSIZE
	int "Qemu ZNS storage nvme logical block size"
	default 4096
	help
	  The logical block size to use for ZNS drives. This ends up what is
	  put into the /sys/block/<disk>/queue/logical_block_size and the
	  smallest unit the storage device can address. It is typically 512
	  bytes.

endif # QEMU_CUSTOM_NVME_ZNS

config VAGRANT_ENABLE_ZNS
	bool
	default y if QEMU_ENABLE_NVME_ZNS

config QEMU_NVME_ZONE_DRIVE_SIZE
	int
	default 102400 if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_DRIVE_SIZE if QEMU_CUSTOM_NVME_ZNS

config QEMU_NVME_ZONE_ZASL
	int
	default 0 if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_ZASL if QEMU_CUSTOM_NVME_ZNS

config QEMU_NVME_ZONE_SIZE
	string
	default "128M" if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_SIZE if QEMU_CUSTOM_NVME_ZNS

config QEMU_NVME_ZONE_CAPACITY
	string
	default "0M" if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_CAPACITY if QEMU_CUSTOM_NVME_ZNS

config QEMU_NVME_ZONE_MAX_ACTIVE
	int
	default 0 if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_MAX_ACTIVE if QEMU_CUSTOM_NVME_ZNS

config QEMU_NVME_ZONE_MAX_OPEN
	int
	default 0 if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_MAX_OPEN if QEMU_CUSTOM_NVME_ZNS

config QEMU_NVME_ZONE_PHYSICAL_BLOCKSIZE
	int
	default 4096 if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_PHYSICAL_BLOCKSIZE if QEMU_CUSTOM_NVME_ZNS

config QEMU_NVME_ZONE_LOGICAL_BLOCKSIZE
	int
	default 4096 if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_LOGICAL_BLOCKSIZE if QEMU_CUSTOM_NVME_ZNS

config QEMU_ENABLE_EXTRA_DRIVE_LARGEIO
	bool "Enable Qemu drives for large IO experimentation"
	depends on VAGRANT_LIBVIRT
	default n
	help
	  If you want to experiment with large IO either with NVMe or virtio
	  you can enable this option. This will create a few additional drives
	  which are dedicated for largio experimentation testing.


	  If unsure say N.

if QEMU_ENABLE_EXTRA_DRIVE_LARGEIO

config QEMU_EXTRA_DRIVE_LARGEIO_BASE_SIZE
	int "Qemu extra drive drive base size"
	default 10240
	help
	  The base size of the qemu extra storage drive to expose. The
	  size is increased by 1 MiB as we go down the list of extra large IO
	  drives.

config QEMU_EXTRA_DRIVE_LARGEIO_COMPAT
	bool "Use a compatibility logical block size"
	default n
	help
	  Since older spindle drives used to work with 512 bytes some drives
	  exist with support to handle 512 writes even if they physically store
	  more data on their drives for that one 512 byte write. Enable this if
	  you want to ensure your large IO drives always have a logical block
	  size restrained by the compatibility size you want to support.

	  By default this is not enabled, and thereore the logical block size
	  for the large IO drives will be equal to the physical block size.

config QEMU_EXTRA_DRIVE_LARGEIO_COMPAT_SIZE
	int "Large IO compat size"
	default 512
	help
	  This is the compatibility base block size to use for older drives.
	  Even if you disable QEMU_EXTRA_DRIVE_LARGEIO_COMPAT, this value will
	  be used as the base for the computation for the physical block size
	  for the large IO drives we create for you using the formula:

	    libvirt_largeio_logical_compat_size  * (2 ** n)

	  where n is the index of the large IO drive.

config QEMU_EXTRA_DRIVE_LARGEIO_MAX_POW_LIMIT
	int "Large IO - number of drives - power"
	default 12
	help
	  We use an iterator to create the number of large drives on the
	  guest system using:

	    for n in range(0,libvirt_largeio_pow_limit)

	  This provides the value for the libvirt_largeio_pow_limit. By
	  default we set this to 12 so we get drives of different physical
	  sizes in powers of 2 ranging from 512 up to 1 GiB. You can reduce
	  this if you want less drives to experiment with.

endif # QEMU_ENABLE_EXTRA_DRIVE_LARGEIO

config VAGRANT_ENABLE_LARGEIO
	bool
	default y if QEMU_ENABLE_EXTRA_DRIVE_LARGEIO

config QEMU_LARGEIO_DRIVE_BASE_SIZE
	int
	default 10240 if !QEMU_ENABLE_EXTRA_DRIVE_LARGEIO
	default QEMU_EXTRA_DRIVE_LARGEIO_BASE_SIZE if QEMU_ENABLE_EXTRA_DRIVE_LARGEIO

config QEMU_LARGEIO_COMPAT_SIZE
	int
	default 512 if !QEMU_ENABLE_EXTRA_DRIVE_LARGEIO
	default QEMU_EXTRA_DRIVE_LARGEIO_COMPAT_SIZE if QEMU_ENABLE_EXTRA_DRIVE_LARGEIO

config QEMU_LARGEIO_MAX_POW_LIMIT
	int
	default 12 if !QEMU_ENABLE_EXTRA_DRIVE_LARGEIO
	default QEMU_EXTRA_DRIVE_LARGEIO_MAX_POW_LIMIT if QEMU_ENABLE_EXTRA_DRIVE_LARGEIO

config QEMU_ENABLE_CXL
	bool "Enable Qemu CXL devices"
	depends on VAGRANT_LIBVIRT
	depends on LIBVIRT_MACHINE_TYPE_Q35
	depends on QEMU_USE_DEVELOPMENT_VERSION
	default n
	help
	  If this option is enabled then you can enable different types of
	  CXL devices which we will emulate for you.

if QEMU_ENABLE_CXL

choice
	prompt "CXL topology to enable"
	default QEMU_ENABLE_CXL_DEMO_TOPOLOGY_1

config QEMU_ENABLE_CXL_DEMO_TOPOLOGY_1
	bool "Basic CXL demo topology with a CXL Type 3 device"
	help
	  This is a basic CXL demo topology. It consists of single host brige that
	  has one root port. A Type 3 persistent memory device is attached to the
	  root port. This topology is referred to as a passthrough decoder in
	  kernel terminology. The kernel CXL core will consume the resource exposed
	  in the ACPI CXL memory layout description, such as Host Managed
	  Device memory (HDM), CXL Early Discovery Table (CEDT), and the
	  CXL Fixed Memory Window Structures to publish the root of a
	  cxl_port decode hierarchy to map regions that represent System RAM,
	  or Persistent Memory regions to be managed by LIBNVDIMM.

config QEMU_ENABLE_CXL_DEMO_TOPOLOGY_2
	bool "Host bridge with two root ports"
	help
	  This topology extends the first demo topology by placing two root ports
	  in the host bridge. This ensures that the decoder associated with the
	  host bridge is not a passthrough decoder.

endchoice

endif # QEMU_ENABLE_CXL

endif # VAGRANT
